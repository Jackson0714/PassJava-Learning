RocketMQ 架构图

## 整体架构



![](http://cdn.jayh.club/uPic/image-20220208093224992Tvv2Rg.png)

**第一块**就是他的NameServer，这个东西很重要，他要负责去管理集群里所有Broker的信息，让使用MQ的系统可以通过他感知到集群

里有哪些Broker。

**第二块**就是Broker集群本身了，必须得在多台机器上部署这么一个集群，而且还得用主从架构实现数据多副本存储和高可用。

**第三块**就是向MQ发送消息的那些系统了，这些系统一般称之为生产者，这里也有很多细节是值得深究的，因为这些生产者到底是如何

从NameServer拉取路由信息的？如何选择Broker机器建立连接以及发送消息的？

**第四块**就是从MQ获取消息的那些系统，这些系统一般称之为消费者。

### MQ如何集群化部署来支撑高并发访问？

![](http://cdn.jayh.club/uPic/image-20220208093846141QvVQXd.png)

RocketMQ是可以集群化部署的，可以部署在多台机器上，假设每台机器都能抗10万并发，然后你只要让几十万请求分散到多

台机器上就可以了，让每台机器承受的QPS不超过10万不就行了。



### MQ如果要存储海量消息应该怎么做？

其次，每台机器上部署的RocketMQ进程一般称之为Broker，每个Broker都会收到不同的消息，然后就会把这批消息存储在自己本地

的磁盘文件里

这样的话，假设你有1亿条消息，然后有10台机器部署了RocketMQ的Broker，理论上不就可以让每台机器存储1000万条消息了吗？



![image-20220208100456146](http://cdn.jayh.club/uPic/image-20220208100456146GafmV2.png)

所以本质上RocketMQ存储海量消息的机制就是分布式的存储

所谓分布式存储，就是把数据分散在多台机器上来存储，每台机器存储一部分消息，这样多台机器加起来就可以存储海量消息了！

### 高可用保障：万一Broker宕机了怎么办？

RocketMQ的解决思路是Broker主从架构以及多副本策略。

Master Broker收到消息之后会同步给Slave Broker，这样Slave Broker上就能有一模一样的一份副本数据！

候如果任何一个Master Broker出现故障，还有一个Slave Broker上有一份数据副本，可以保证数据不丢失，还能继续对外提供

服务，保证了MQ的可靠性和高可用性。

### 数据路由：怎么知道访问哪个Broker？

怎么知道要连接到哪一台Broker上去发送和接收消息？

有一个NameServer的概念，他也是独立部署在几台机器上的，然后所有的Broker都会把自己注册

到NameServer上去，NameServer不就知道集群里有哪些Broker了？

## NameServer

每个 Broker 启动都得向所有的 NameServer 进行注册

每个 Master Broker得向两台 NameServer 都进行注册的情况。

![](http://cdn.jayh.club/uPic/image-20220208110002299VthLY0.png)

生产者和消费者系统主动去NameServer拉取Broker信息的。

### 如果某个 Broker 宕机了，怎么办？

Broker 和 NameServer 之间有心跳机制，Broker 每隔 30s 给所有的 NameServer 发送心跳，告诉每隔 NameServer 自己目前还活着。

每次NameServer收到一个Broker的心跳，就可以更新一下他的最近一次心跳的时间，然后NameServer会每隔10s运行一个任务，去检查一下各个Broker的最近一次心跳时间，如果某个Broker超过120s都没发送心跳了，那么就认为这个Broker已经挂掉了。

### Broker 宕机，系统是怎么感知到的？

- 跟 Slave 通信？

- 过一会重新从 NameServer 拉取到最新的路由信息，此时就知道有一个 Broker 已经宕机了。

### 其他 MQ 的路由中心

Kafka 的路由中心实际上是一个非常复杂、混乱的存在。他是由ZooKeeper以及某个作为Controller的Broker共同完成的。

RabbitMQ 由集群每个节点同时扮演了路由中心的角色。

RocketMQ是把路由中心抽离出来作为一个独立的NameServer角色运行的，因此可以说在路由中心这块，他的架构设计是最清晰明

了的。

> 问题：如果 NameServer 服务宕机了，RocketMQ 还能正常工作吗？生产者还能发送消息到 Broker 吗？消费者还能从 Broker 拉取消息吗？

## Broker

高可用，将 Broker 部署成 Master-Slave 模式。一对一的。

Master 接收到消息后，将数据同步给 Slave，Master Broker 挂了后，Slave 上有一份数据。

Slave Broker 会向所有的 NameServer 进行注册，也会每 30s 发送一次心跳。

Slave Broker 不停地发送请求到 Master Broker 去拉取消息。Pull 模式。

写入消息：选择 Master Broker 去写入。

读取消息：有可能从Master Broker获取消息，也有可能从Slave Broker获取消息。根据当时Master Broker的负载情况和Slave Broker的同步情况。

### 如果Slave Broke挂掉了有什么影响？

如果Slave Broker挂了，那么此时无论消息写入还是消息拉取，还是可以继续从Master Broke去走，对整体运行不影响，但是会导致所有读写压力都集中在Master Broker上

### 如果Master Broker挂掉了该怎么办？

在RocketMQ 4.5版本之前，都是用Slave Broker同步数据，Master-Slave 模式**不是彻底的高可用模式，他没法实现自动把Slave切换为Master**。需要手动做一些运维操作，把Slave Broker重新修改一些配置，重启机器给调整为Master Broker，这是有点麻烦的，而且会导致中间一段时间不可用。

在RocketMQ 4.5之后，在多个Slave中，通过Dledger技术和Raft协议算法进行leader选举，直接将一个Slave Broker选举为新的Master Broker。

### Broker是如何跟NameServer进行通信的？

TCP长连接：Broker会跟每个NameServer都建立一个TCP长连接，然后定时通过TCP长连接发送心跳请求过去。

各个NameServer就是通过跟Broker建立好的长连接不断收到心跳包，然后定时检查Broker有没有120s都没发送心跳包，来判定集群里各个Broker到底挂掉了没有。

![](http://cdn.jayh.club/uPic/image-20220208200603435cpIAlX.png)

### Broker 怎么存储的？

CommitLog消息顺序写入机制：

把这个消息直接写入磁盘上的一个日志文件，叫做CommitLog，直接顺序写入这个文件。

CommitLog是很多磁盘文件，每个文件限定最多1GB，Broker收到消息之后就直接追加写入这个文件的末尾。如果一个CommitLog写满了1GB，就会创建一个新的CommitLog文件。

![](http://cdn.jayh.club/uPic/image-20220210112035702o0kHFa.png)



MessageQueue在数据存储中是体现在哪里呢？

在Broker的磁盘上，对Topic下的每个MessageQueue都会有一系列的ConsumeQueue文件。

ConsumeQueue 文件的路径如下：

```SH
$HOME/store/consumequeue/{topic}/{queueId}/{fileName}
```

TopicOrder Topic 有两个 message queue：MessageQueue0 和 MessageQueue1，分别对应 ConsumeQueue0、ConsumeQueue1、ConsumeQueue2 磁盘文件

$HOME/store/consumequeue/TopicOrder/MessageQueue0/ConsumeQueue0 磁盘文件

$HOME/store/consumequeue/TopicOrder/MessageQueue0/ConsumeQueue1 磁盘文件

$HOME/store/consumequeue/TopicOrder/MessageQueue1/ConsumeQueue2 磁盘文件

当你的Broker收到一条消息写入了CommitLog之后，其实他同时会将这条消息在CommitLog中的物理位置，也就是一个文件偏移量，就是一个offset，写入到这条消息所属的MessageQueue对应的ConsumeQueue文件中去。

在ConsumeQueue中存储的每条数据不只是消息在CommitLog中的offset偏移量，还包含了消息的长度，以及taghashcode，一条数据是20个字节，每个ConsumeQueue文件保存30万条数据，大概每个文件是5.72MB。

![](http://cdn.jayh.club/uPic/image-20220210112909501fVJOiC.png)

Topic中的多个MessageQueue会分散在多个Broker上，在每个Broker机器上，一个MessageQueue就对应了一个ConsumeQueue，当然在物理磁盘上其实是对应了多个ConsumeQueue文件的，但是我们大致也理解为一 一对应关系。





### Broker 的高吞吐

磁盘文件顺序写+OS PageCache写入+OS异步刷盘的策略

顺序写：每次写入就是在文件末尾追加一条数据就可以了，对文件进行顺序写的性能要比对文件随机写的性能提升很多。

PageCache：数据先写入OS的PageCache缓存中，然后后续由OS自己的线程将缓存里的数据刷入磁盘中。

## DLedger 技术

使用DLedger来管理CommitLog，然后Broker还是可以基于DLedger管理的CommitLog去构建出来机器上的各个ConsumeQueue磁盘文件。

![](http://cdn.jayh.club/uPic/image-20220210120657554EuhJ6N.png)

每个Broker上都有一个DLedger组件。

### 同步刷盘和异步刷盘

异步刷盘：高吞吐，但是有数据丢失的风险。

同步刷盘：低吞吐，数据不丢失。

![](http://cdn.jayh.club/uPic/image-20220210114046429c2ZT7C.png)

### Learder 选举和同步

多副本同步+Leader自动切换

基于DLedger技术管理CommitLog

Broker集群启动时，基于DLedger技术和Raft协议完成Leader选举

如果Leader Broker崩溃，则基于DLedger和Raft协议重新选举Leader。

Raft 协议：他确保有人可以成为Leader的核心机制就是一轮选举不出来Leader的话，就让大家随机休眠一下，先苏醒过来的人会投票给自己，其他人苏醒过后发现自己收到选票了，就会直接投票给那个人。依靠这个随机休眠的机制，基本上几轮投票过后，一般都是可以快速选举出来一个Leader。

### 同步数据到副本：两阶段提交

Leader Broker写入之后，基于DLedger技术和Raft协议同步数据给Follower Broker

数据同步会分为两个阶段，一个是uncommitted阶段，一个是commited阶段。

首先Leader Broker上的DLedger收到一条数据之后，会标记为uncommitted状态，然后他会通过自己的DLedgerServer组件把这个uncommitted数据发送给Follower Broker的DLedgerServer。

接着Follower Broker的DLedgerServer收到uncommitted消息之后，必须返回一个ack给Leader Broker 的  DLedgerServer，然后如果Leader Broker收到超过半数的Follower Broker返回ack之后，就会将消息标记为committed状态。

然后Leader Broker上的DLedgerServer就会发送commited消息给Follower Broker机器的DLedgerServer，让他们也把消息标记为comitted状态。

这个就是基于Raft协议实现的两阶段完成的数据同步机制。

## 如何高可用部署

![](http://cdn.jayh.club/uPic/image-20220208202923578N2azNK.png)

## Topic

Topic 代表数据集合。在发送消息的时候指定你要发送到哪个Topic里面去。

一个订单Topic的数据分布式存储在两个Master Broker上了。

Broker心跳的时候会汇报给NameServer自己的数据情况，这样每个NameServer都知道集群里有哪些Broker，每个Broker存放了哪些Topic的数据。

### 生产者如何发送消息

生产者系统自然就可以通过路由信息找到自己要投递消息的Topic分布在哪几台Broker上，此时可以根据负载均衡算法，从里面选择一台Broke机器出来。选择一台Broker之后，就可以跟那个Broker也建立一个TCP长连接，然后通过长连接向Broker发送消息即可。Broker收到消息之后就会存储在自己本地磁盘里去。

### 消费者如何拉取消息

消费者系统其实跟生产者系统原理是类似的，他们也会跟NameServer建立长连接，然后拉取路由信息，接着找到自己要获取消息的Topic在哪几台Broker上，就可以跟Broker建立长连接，从里面拉取消息了。消费者系统可能会从Master Broker拉取消息，也可能从Slave Broker拉取消息。

这套架构还具备伸缩性，就是说如果要抗更高的并发，存储跟多的数据，完全可以在集群里加入更多的Broker机器，这样就可以线性扩展集群了。

## MessageQueue

### Topic、MessageQueue以及Broker之间到底是什么关系？

![](http://cdn.jayh.club/uPic/image-20220210102342155uPoTLS.png)

MessageQueue就是RocketMQ中非常关键的一个数据分片机制，他通过MessageQueue将一个Topic的数据拆分为了很多个数据分片，然后在每个Broker机器上都存储一些MessageQueue。

### 生产者发送消息的时候写入哪个MessageQueue？

以生产者从NameServer中就会知道，一个Topic有几个MessageQueue，哪些MessageQueue在哪台Broker机器上，哪些MesssageQueue在另外一台Broker机器上。

## 消费消息

### MessageQueue与消费者的关系

一个Topic上的多个MessageQueue，是如何由一个消费组中的多台机器来进行消费的呢？

一个Topic的多个MessageQueue会均匀分摊给消费组内的多个机器去消费，这里的一个原则就是，一个

MessageQueue只能被一个消费机器去处理，但是一台消费者机器可以负责多个MessageQueue的消息处理。

### Broker 取哪些消息给消费者？

是=根据你要消费的MessageQueue以及开始消费的位置，去找到对应的ConsumeQueue读取里面对应位置的消息在CommitLog中的物理offset偏移量，然后到CommitLog中根据offset读取消息数据，返回给消费者机器。

### 消费者处理消息

消费者拉取到消息后，就会回调注册的函数：

![](http://cdn.jayh.club/uPic/image-20220211103833736pKkxCe.png)

处理完消息后，消费者将`消费者进度`提交到 Broker 上，Broker 存储我们的消费进度。

比如我们现在对ConsumeQueue0的消费进度假设就是在offset=1的位置，那么他会记录下来一个ConsumeOffset的东西去标记我们的消费进度，如下图。

![](http://cdn.jayh.club/uPic/image-20220211104216323x85t5Y.png)

下次这个消费组只要再次拉取这个ConsumeQueue的消息，就可以从Broker记录的消费位置开始继续拉取，不用重头开始拉取了。

### Push 模式和 Pull 模式

使用 RocketMQ 时，消费者模式一般都是用 Push 模式。因为 Pull 模式的代码写起来更复杂和繁琐，而且 Push 模式底层本身就是基于 Pull 的模式，只是时效性更好，看起来像是 Broker 不断 Push 消息给消费者。

Push 模式：当消费者发送请求到Broker去拉取消息的时候，如果有新的消息可以消费那么就会立马返回

一批消息到消费机器去处理，处理完之后会接着立刻发送请求到Broker机器去拉取下一批消息。

消息处理的时效性非常好，看起来就跟Broker一直不停的推送消息到消费机器一样。

请求挂起和长轮询的机制：消费者发送请求到 Broker 后，如果没有新消息，则让请求线程挂起，默认是挂起 15 秒。Broker 有后台线程每隔一会检查下是否有新消息，如果有，则唤醒挂起的线程，把消息给消费者。

### 如果某个Broker出现故障该怎么办？

![](http://cdn.jayh.club/uPic/image-20220210111152837kxKDIg.png)

在Producer中开启一个开关，就是sendLatencyFaultEnable。

自动容错机制，比如如果某次访问一个Broker发现网络延迟有500ms，然后还无法访问，那么就会自动回避访问这个Broker一段时间，比如接下来3000ms内，就不会访问这个Broker了。

过一段时间之后，可能这个Master Broker就已经恢复好了，比如他的Slave Broker切换为了Master可以让别人访问了。

### 消费者根据什么策略从 Master 或 Slave 上拉取消息？

消费者都是连接到Master Broker机器去拉取消息的，然后如果Master Broker机器觉得自己负载比较高，就会告诉消费者机器，下次可以从Slave Broker机器去拉取。

写入CommitLog时先进入os cache缓存，而不是直接进入磁盘的机制。

是ConsumeQueue会被大量的消费者发送的请求给高并发的读取。broker对ConsumeQueue文件同样也是基于os cache来进行优化的。

从CommitLog里读取消息完整数据是如何读取的？是从os cache里读取？还是从磁盘里读取？

**第一种可能**，如果你读取的是那种刚刚写入CommitLog的数据，那么大概率他们还停留在os cache中，此时你可以顺利的直接从oscache里读取CommitLog中的数据，这个就是内存读取，性能是很高的。

**第二种可能**，你也许读取的是比较早之前写入CommitLog的数据，那些数据早就被刷入磁盘了，已经不在os cache里了，那么此时你就只能从磁盘上的文件里读取了，这个性能是比较差一些的。

本质是对比你当前没有拉取消息的数量和大小，以及最多可以存放在os cache内存里的消息的大小，如

果你没拉取的消息超过了最大能使用的内存的量，那么说明你后续会频繁从磁盘加载数据，此时就让你从slave broker去加载数据了！

## 消费者组

一组消费者的统一用一个名字。比如我们有一个 Topic：TopicOrderSuccess，订单系统和库存系统订阅了这个消息，这两个系统都有一个自己的消费者组：order_consumer_group，stock_consumer_group，自己系统的消费者对应到这个消费者组里面。

![](http://cdn.jayh.club/uPic/image-20220211095032789CM5Uvl.png)

默认情况下我们都是`集群模式`，也就是说，一个消费组获取到一条消息，只会交给组内的一台机器去处理，不是每台机器都可以获取到这条消息的。

如下设置来改变为`广播模式`：

consumer.setMessageModel(MessageModel.BROADCASTING);

如果修改为广播模式，那么对于消费组获取到的一条消息，组内每台机器都可以获取到这条消息。但是相对而言广播模式其实用的很少，常见基本上都是使用集群模式来进行消费的。

### **如果消费组中出现机器宕机或者扩容加机器，会怎么处理？**

负载重平衡的

## 集群压测

### 监控工具

### RocketMQ 可视化监控工具

git clone https://github.com/apache/rocketmq-externals.git

Zabbix、Open-Falcon、Linux 命令，监控机器的性能和资源使用率

### OS 内核参数调整

（1）vm.overcommit_memory

可选值：0、1、2。

0 表示当系统申请内存时，OS 内核检查可用内存是否足够，如果足够就分配，如果不够就拒绝申请请求，导致申请失败，中间件系统出现异常报错。

1 表示所有可用的物理内存都允许分配。

```
echo 'vm.overcommit_memory=1' >> /etc/sysctl.conf
```

（2）vm.max_map_count

控制中间件系统可以开启的线程的数量。

默认值 65536，建议调大 10 倍

```
echo 'vm.max_map_count=655360' >> /etc/sysctl.conf
```

（3）vm.swappiness

控制进程的swap行为，os会把一部分磁盘空间作为swap区域，然后如果有的进程现在不是太活跃，就会被操作系统把进程调整为睡眠状态，把进程中的数据放入磁盘上的swap区域，然后让这个进程把原来占用的内存空间腾出来，交给其他活跃运行的进程来使用。

如果设置为 0，则表示尽量别把任何一个进程放到磁盘swap区域去，尽量大家都用物理内存。

如果设置为 100，尽量把一些进程给放到磁盘swap区域去，内存腾出来给活跃的进程使用。

默认值 60，偏高，可能导致我们的中间件运行不活跃的时候被迫腾出内存空间然后放到磁盘swap区域去。

建议设置为 10，尽量用物理内存，不要放磁盘 swap 区域。

```
echo 'vm.swappiness=10' >> /etc/sysctl.conf
```

（4）ulimit

网络通信和磁盘文件 IO 跟这个参数相关。

来控制linux上的最大文件链接数。大量频繁的读写磁盘文件的时候，或者是进行网络通信的时候，都会用到这个参数。

默认值 1024，肯定不够，建议设置为 100000。

```
echo 'ulimit -n 1000000' >> /etc/profile
```

### JVM 参数调整

runbroker.sh脚本 中配置了 JVM 启动参数。

-Xms8g -Xmx8g -Xmn4g：默认的堆大小是8g内存，新生代是4g内存，可以根据机器配置进行调整。

### 压测目标

要压测出来一个最合适的最高负载。最主要的是综合TPS以及机器负载，尽量找到一个最高的TPS同时机器的各项负载在可承受范围之内。

到底应该如何压测：应该在TPS和机器的cpu负载、内存使用率、jvm gc频率、磁盘io负载、网络流量负载之间取得一个平衡，尽量让

TPS尽可能的提高，同时让机器的各项资源负载不要太高。

### 压测小结

**到底应该如何压测**：应该在TPS和机器的cpu负载、内存使用率、jvm gc频率、磁盘io负载、网络流量负载之间取得一个平衡，尽量让TPS尽可能的提高，同时让机器的各项资源负载不要太高。

**实际压测过程**：采用几台机器开启大量线程并发读写消息，然后观察TPS、cpu load（使用top命令）、内存使用率（使用free命令）、jvm gc频率（使用jstat命令）、磁盘io负载（使用top命令）、网卡流量负载（使用sar命令），不断增加机器和线程，让TPS不断提升上去，同时观察各项资源负载是否过高。

**生产集群规划**：根据公司的后台整体QPS来定，稍微多冗余部署一些机器即可，实际部署生产环境的集群时，使用高配置物理机，同时合理调整os内核参数、jvm参数、中间件核心参数





## 其他

### 消息队列应用

#### 如何将数据同步给大数据团队？

先是考虑在订单系统代码内部嵌入一些额外的代码，将订单的增删改操作发送到

RocketMQ里，但是后来发现这样会导致污染订单系统的代码。

所以后来我们提出了一个完美的解决方案，就是用Canal、Databus这样的MySQL Binlog同步系统，监听订单数据库的binlog发送到

RocketMQ里

然后大数据团队的数据同步系统从RocketMQ里获取订单数据的增删改binlog日志，还原到自己的数据存储中去，可以是自己的数据

库，或者是Hadoop之类的大数据生态技术。

然后大数据团队将完整的订单数据还原到自己的数据存储中，就可以根据自己的技术能力去出数据报表了，不会再影响订单系统的数据

库了

## Netty

### Reactor 线程池

Reactor线程池里默认是3个线程！

Producer 发送一个消息过来到达Broker里的 SocketChannel，此时Reactor线程池里的一个线程会监听到这个 SocketChannel 中有请求到达了！

![](http://cdn.jayh.club/uPic/image-20220211193245999Y5atHd.png)

### Worker线程池

接着Reactor线程从SocketChannel中读取出来一个请求，这个请求在正式进行处理之前，必须就先要进行一些准备工作和预处理，比如SSL加密验证、编码解码、连接空闲检查、网络连接管理，诸如此类的一些事。

Worker线程池，他默认有8个线程，此时Reactor线程收到的这个请求会交给Worker线程池中

的一个线程进行处理，会完成上述一系列的准备工作。

![](http://cdn.jayh.club/uPic/image-20220211202007313M64Tmm.png)

Reactor主线程在端口上监听Producer建立连接的请求，建立长连接

Reactor线程池并发的监听多个连接的请求是否到达

Worker线程并发的对多个请求进行预处理

业务线程池并发的对多个请求进行磁盘读写业务操作

### 高性能网络通信架构

一旦连接建立好之后，大量的长连接均匀的分配给Reactor线程池里的多个线程。

每个Reactor线程负责监听一部分连接的请求，这个也是一个优化点，通过多线程并发的监听不同连接的请求，可以有效的提升大量并发请求过来时候的处理能力，可以提升网络框架的并发能力。

## mmap

Broker读写磁盘文件的核心技术

传统普通IO的问题，有两次数据拷贝问题。

读取磁盘文件：首先从磁盘上把数据读取到内核IO缓冲区里去，然后再从内核IO缓存区里读取到用户进程私有空间里去，然后我们才能拿到这个文件里的数据

写磁盘文件：必须先把数据写入到用户进程私有空间里去，然后从这里再进入内核IO缓冲区，最后进入磁盘文件里去

![](http://cdn.jayh.club/uPic/image-20220211205129405W7A4iE.png)

### Broker 基于mmap技术+pagecache技术实现高性能的文件读写

基于JDK NIO包下的MappedByteBuffer的map()函数，来先将一个磁盘文件（比如一个CommitLog文件，或者是一个ConsumeQueue文件）映射到内存里来。

mmap技术在进行文件映射的时候，一般有大小限制，在1.5GB~2GB之间。所以RocketMQ才让CommitLog单个文件在1GB，ConsumeQueue文件在5.72MB，不会太大。

PageCache，实际上在这里就是对应于虚拟内存。

接下来就可以对这个已经映射到内存里的磁盘文件进行读写操作了，比如要写入消息到CommitLog文件，你先把一个CommitLog文件通过MappedByteBuffer的map()函数映射其地址到你的虚拟内存地址。

接着就可以对这个MappedByteBuffer执行写入操作了，写入的时候他会直接进入PageCache中，然后过一段时间之后，由os的线程异步刷入磁盘中。

![](http://cdn.jayh.club/uPic/image-20220211210007365xxGIzD.png)

如果我们要从磁盘文件里读取数据呢？

当前你要读取的数据是否在PageCache里？如果在的话，就可以直接从PageCache里读取了！

而且PageCache技术在加载数据的时候**，**还会将**你加载的数据块的临近的其他数据块也一起加载到PageCache里去。**

![](http://cdn.jayh.club/uPic/image-20220211210124556Wszpu2.png)

Broker 做的优化：

内存预映射机制 + 文件预热

## RocketMQ 丢失消息的场景

- 生产者系统推送支付消息时，因为网络故障，推送失败。
- 消息队列自己丢失消息，消息进入了 os cache，还没写入磁盘，broker 机器故障。
- 消息队列自己丢失消息，磁盘坏了，数据丢失。
- 消费者系统故障，丢失消息。消费者拿到了消息，提交了 offset，Broker 标记为已处理，但是消费者系统自己还没有处理，这个时候消费者系统故障了。

### 发送消息零丢失方案

![](http://cdn.jayh.club/uPic/image-20220215105725728XR0V3l.png)

解决方案：发送 half 消息，RocketMQ 返回 half 消息响应，订单系统执行本地事务，订单系统 rollback 或 commit，RocketMQ 对消息进行 commit。如下图所示：

![](http://cdn.jayh.club/uPic/image-20220215105648748eLoyqU.png)

对于事务消息机制之下的half消息，RocketMQ是写入内部Topic的ConsumeQueue的，不是写入你指定的OrderPaySuccessTopic的ConsumeQueue的。如下图所示：

![](http://cdn.jayh.club/uPic/image-20220215223834191Bwvmql.png)

后台有定时任务，定时任务会去扫描RMQ_SYS_TRANS_HALF_TOPIC中的half消息，如果你超过一定时间还是

half消息，他会回调订单系统的接口，让你判断这个half消息是要rollback还是commit。如下图所示：

![](http://cdn.jayh.club/uPic/image-20220215224056981kUVqGP.png)

如果你执行rollback，他的本质就是用一个OP操作来标记half消息的状态

RocketMQ内部有一个OP_TOPIC，此时可以写一条rollback OP记录到这个Topic里，标记某个half消息是rollback了。如下图所示：

![](http://cdn.jayh.club/uPic/image-20220215224310257BSuRTQ.png)

假设你一直没有执行commit/rollback，RocketMQ会回调订单系统的接口去判断half消息的状态，但是他最多就是回调15次，如果15次之后你都没法告知他half消息的状态，就自动把消息标记为rollback。

你执行commit操作之后，RocketMQ就会在OP_TOPIC里写入一条记录，标记half消息已经是commit状态了。

接着需要把放在RMQ_SYS_TRANS_HALF_TOPIC中的half消息给写入到OrderPaySuccessTopic的ConsumeQueue里去，然后我们的红包系统可以就可以看到这条消息进行消费了。如下图所示：

![](http://cdn.jayh.club/uPic/image-20220215224432248Uiew5d.png)

消息零丢失方案，会有性能问题。

## 重复消费消息

如果有接口超时等问题，可能会导致上游的支付系统重试调用订单系统的接口，进而导致订单系统对一个消息重复发送两条到MQ里去！

假设你发送了一条消息到MQ了，其实MQ是已经接收到这条消息了，结果MQ返回响应给你的时候，网络有问题超时了，就是你没能及时收到MQ返回给你的响应。

没有重复发送消息到MQ，哪怕MQ里就一条消息，优惠券系统也有可能会重复进行消费。假设你的优惠券系统拿到了一条订单成功支付的消息，然后都已经进行处理了，也就是说都已经对这个订单给你发了一张优惠券了，本我们之前讲过，这个时候他应该返回一个CONSUME_SUCCESS的状态，然后提交消费进度offset到broker的。但是不巧的是，你刚刚发完优惠券，还没来得及提交消息offset到broker呢！优惠券系统就进行了一次重启！比如可能优惠券系统的代码更新了，需要重启进行重新部署。

解决方案：消费者系统使用幂等性方案。

要保证消息的幂等性，我们优先推荐的其实还是业务判断法，直接根据你的数据存储中的记录来判断这个消息是否处理过，如果处理过了，那就别再次处理了。因为我们要知道，基于Redis的消息发送状态的方案，在一些极端情况下还是没法完全保证幂等性的。