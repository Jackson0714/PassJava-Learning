(21 条消息) SpringCloud Alibaba(四) Nacos 服务端本地启动和源码浅析 (AP 架构),Distro 协议介绍, CAP 原则介绍_Dr. 劳的博客 - CSDN 博客

  前一章[ SpringCloud Alibaba(三) Nacos 客户端源码浅析 (AP 架构)](https://blog.csdn.net/qq_24768941/article/details/122334104), 我们学习了 [Nacos](https://so.csdn.net/so/search?q=Nacos&spm=1001.2101.3001.7020) 的客户端, 我们知道了客户端会调用服务端的接口, 包括注册到注册中心, 心跳保活, 拉取服务列表. 这一章我们来看一下 Nacos 服务端, 也是基于 Nacos1.4.1(21 年初) 版本, 尽管现在已经出了 2.0 版本, 他们之间最大的改变是 1.X 的 Http 请求, 2.X 使用的是 grpc, 但是市面上用得最多的仍然是 1.X 版本, 我们只需要学会他的思想 , 万变不离其宗. Spring Cloud 版本为 Hoxton.SR8,Spring Cloud Alibaba 版本为 2.2.5.RELEASE.



# 1. 基础知识



## 1.1 CAP 原则



  CAP 定理: 指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可同时获得



- 一致性（C）：所有节点都可以访问到最新的数据
- 可用性（A）：每个请求都是可以得到响应的，不管请求是成功还是失败
- 分区容错性（P）：除了全部整体网络故障，其他故障都不能导致整个系统不可用



  CAP 理论就是说在分布式存储系统中，最多只能实现上面的两点。而由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。所以我们只能在一致性和可用性之间进行权衡



  所谓的一致性并不是指集群中所有节点在任一时刻的状态必须完全一致，而是指一个目标，即让一个分布式系统看起来只有一个数据副本，并且读写操作都是原子的，这样应用层就可以忽略系统底层多个数据副本间的同步问题。也就是说，我们可以将一个强一致性（线性一致性）分布式系统当成一个整体，一旦某个客户端成功的执行了写操作，那么所有客户端都一定能读出刚刚写入的值。即使发生网络分区故障，或者少部分节点发生异常，整个集群依然能够像单机一样提供服务。





```java
/**
     * Update instances.
     *
     * @param instances instances
     * @param ephemeral whether is ephemeral instance
     */
    public void updateIPs(Collection<Instance> instances, boolean ephemeral) {
       // 创建一个临时的服务对应的map。
        Map<String, List<Instance>> ipMap = new HashMap<>(clusterMap.size());
        for (String clusterName : clusterMap.keySet()) {
            ipMap.put(clusterName, new ArrayList<>());
        }
      // 遍历instance列表
        
        for (Instance instance : instances) {
            try {
                if (instance == null) {
                    Loggers.SRV_LOG.error("[NACOS-DOM] received malformed ip: null");
                    continue;
                }
                // 设置cluster名称
                if (StringUtils.isEmpty(instance.getClusterName())) {
                    instance.setClusterName(UtilsAndCommons.DEFAULT_CLUSTER_NAME);
                }
                // 创建一个空的Cluster，设置到clusterMap中
                if (!clusterMap.containsKey(instance.getClusterName())) {
                    Loggers.SRV_LOG
                            .warn("cluster: {} not found, ip: {}, will create new cluster with default configuration.",
                                    instance.getClusterName(), instance.toJson());
                    Cluster cluster = new Cluster(instance.getClusterName(), this);
                    cluster.init();
                    getClusterMap().put(instance.getClusterName(), cluster);
                }
                
                List<Instance> clusterIPs = ipMap.get(instance.getClusterName());
                if (clusterIPs == null) {
                    clusterIPs = new LinkedList<>();
                    ipMap.put(instance.getClusterName(), clusterIPs);
                }
                // 将instance添加到ipMap中此instance对应的clustername位置上
                clusterIPs.add(instance);
            } catch (Exception e) {
                Loggers.SRV_LOG.error("[NACOS-DOM] failed to process ip: " + instance, e);
            }
        }
        // 遍历ipMap,,从clusterMap获取节点包含了老的和最新的Instance。将instance对应的cluster更新到clusterMap中。
        for (Map.Entry<String, List<Instance>> entry : ipMap.entrySet()) {
            //make every ip mine
            List<Instance> entryIPs = entry.getValue();
            // 真正的更新服务列表
            clusterMap.get(entry.getKey()).updateIps(entryIPs, ephemeral);
        }
        
        setLastModifiedMillis(System.currentTimeMillis());
        // 服务列表更新了，发生变化，发布推送事件，触发服务端向客户端的推送任务
        getPushService().serviceChanged(this);
        StringBuilder stringBuilder = new StringBuilder();
        
        for (Instance instance : allIPs()) {
            stringBuilder.append(instance.toIpAddr()).append("_").append(instance.isHealthy()).append(",");
        }
        
        Loggers.EVT_LOG.info("[IP-UPDATED] namespace: {}, service: {}, ips: {}", getNamespaceId(), getName(),
                stringBuilder.toString());
        
    }
```

Nacos 集群同步数据即可以有 AP 模式也可以有 CP 模式

CA 模式: 单机的 mysql

CP 模式: Nacos,Zookeeper

AP 模式: Nacos,Eureka



  下面我们来看一下为什么分布式环境下 CA 是冲突的. 下图左有 Nacos1,Nacos2,Nacos3,3 个节点组成的 Nacos 集群 (注册中心集群), 我们的 client 写 data 到集群, 他会先写到 Nacos1,Nacos1 会把 data 同步到 Nacos2 和 Nacos3, 同步完成才返回给客户端说写入 data 成功, 这样即使我们的 client 去每一个节点读数据, 都是能读到一样的数据, 如果有一个节点没成功, 则报错, 这样是不是可以满足我们的 C, 一致性.



  突然由于网络的问题 Nacos1 和 Nacos2 节点是可以正常通信, 但是 Nacos3 却不可以和他们两个节点通信, 这个时候就产生了 2 个区域. 产生分区的时候, 如果整个集群不可用, 那你这个集群太脆弱了, 一点网络问题就会导致集群不可以用, 所以分布式系统一定需要满足 P, 即使发生部分分区, 仍然可以对外进行服务.



  假设现在发生了分区, 为了保证可用性 A, 那我的客户端写入 data 的时候, 不可能写到 Nacos3 节点, 因为网络不通畅, 即使写入了 Nacos1 也同步不过去 Nacos3, 所以整个集群保证不了一致性 C, 只有等网络恢复, 我们的 Nacos3 再去 Nacos1 拉取数据, 达到最终一致. 如果你硬要保持一致性 C, 那只能整个集群不能对外提供服务, 等 Nacos3 恢复网络, 再提供服务, 这和我们的可用性 A 产生冲突.



  所以我们得出结论, 在分布式存储环境下, CA 是有冲突的.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16Stmp4s.png)





### 1.1.1 脑裂



- 是指在多机房（网络分区）部署中，若出现网络连接问题，形成多个分区，则可能出现脑裂问题，会导致数据不一致。
- 以下图为例 (Nacos CP 模式), 假设我的 Nacos1,Nacos2,Nacos3 是一个集群, CP 模式下, 会有一个主节点, 假设 Nacos1 是 leader(领导, 集群的大脑), 他负责写数据以及同步数据到 Nacos2,3. 现在发生了分区, Nacos3 被独立出来了, 这个时候 Nacos3 发现我自己变成一个区域了, 这个区域还没有 leader, 然后把自己选为了 leader, 这就是脑裂.
- . 这个时候 Nacos1 和 Nacos3 都是 leader. 我们假设这个时候 client 可以往 Nacos1 和 Nacos3 两个集群写数据 (Nacos1 集群和 Nacos3 集群网络是不通的), 那他一会写 1 一会写 3, 就会造成整个集群数据不一致, 网络恢复的时候数据要怎么解决冲突呢?
- 网络恢复的时候已经分不清哪些数据的变化, 如果强行合并显然这不是一个很好的方法, 所以 Nacos(cp) 和 zookeeper 会有一个过半选举机制, 当 Nacos3 想把自己选为 leader 的时候, 需要得到半数以上节点的投票, 现在集群 3 个节点, 需要得到 2 票他才可以选自己为 leader, 这个时候分区了, 显然 Naces3 是不可能得到 2 票的, 这个时候我们的 Nacos3 不应该对外提供服务, 直到网络恢复, 然后 Nacos3 去 Nacos1 主节点, 把最新的数据给拉下来



## ![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16Stmp4s.png)







## 1.2 BASE 理论



什么是 Base 理论



- CAP 中的一致性和可用性进行一个权衡的结果，核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性, 来自 ebay 的架构师提出, 就是 CA 两个特性的权衡.
- Basically Available(基本可用)
  - 假设系统，出现了不可预知的故障，但还是能用, 可能会有性能或者功能上的影响
- Soft state（软状态）
  - 允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时
- Eventually consistent（最终一致性）
  - 系统能够保证在没有其他新的更新操作的情况下，数据最终一定能够达到一致的状态，因此所有客户端对系统的数据访问最终都能够获取到最新的值
- 以上图为例 client 写数据 data 到 Nacos1 的时候我不再需要等数据同步完成到 Nacos2,3 再返回成功了, 即使写入 Nacos2,3 失败了也不影响, 所以 1 和 2,3 的数据会有延迟或者这个时候集群的节点里面数据可能不一致, 这就是软状态, 但是最终会一致. 就算 2,3 节点挂了, 我 1 节点仍然可用这就是基本可用, 并且 2,3 节点恢复之后, 我可以把数据同步回来, 达到最终一致.
- 他牺牲了强一致性来获得可用性, 并且最终一致.



## 总结



在进行分布式系统设计和开发时，我们不应该仅仅局限在 CAP 问题上，还要关注系统的扩展性、可用性等等



在系统发生 “分区” 的情况下，CAP 理论只能满足 CP 或者 AP。要注意的是，这里的前提是系统发生了“分区”



如果系统没有发生 “分区” 的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。



总结：**如果系统发生 “分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区” 的话，我们要思考如何保证 CA 。**





## 1.3 Distro 协议



   作为注册中心, P 要保证, C 和 A 需要权衡; 常见的一致性协议有 paxos、zab、raft, 他们都是强一致性协议 (CP), 然而 nacos 的 distro 协议时弱一致协议（AP）, 即最终一致性协议.



  当然 nacos 也使用了 raft 实现了 CP 模式, 但是作为注册中心, 可用性比一致性更重要, 所以 CP 模式很少用, 我可以允许他暂时不一致, 但是最终一致. 这里推荐文章[阿里巴巴为什么不用 ZooKeeper 做服务发现？](https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651007830&idx=1&sn=7382412cd4a2243b34f69c3cf4aa5a20&scene=21)



  总结来说 Distro 协议是一个让集群中数据达到最终一致的一个协议, 他是 Nacos AP 模式下每个节点直接同步数据的一个协议, 规范.



## 1.4 Nacos AP/CP 的配套一致性协议



需要回顾 Nacos 中的两个概念：临时服务和持久化服务。



- 临时服务（Ephemeral）：临时服务健康检查失败后会从列表中删除，常用于服务注册发现场景。
- 持久化服务（Persistent）：持久化服务健康检查失败后会被标记成不健康，常用于 DNS 场景。



两种模式使用的是不同的一致性协议：



- 临时服务使用的是 Nacos 为服务注册发现场景定制化的私有协议 distro，其一致性模型是 AP；
- 而持久化服务使用的是 raft 协议，其一致性模型是 CP。



配置文件配置是否临时实例, 默认不写为 true



```
/**
     * Update instance list.
     *
     * @param ips       instance list
     * @param ephemeral whether these instances are ephemeral
     */
//ips这个参数包含了当前Nacos内存里面当前节点的所有Instance,包含我新注册的
    public void updateIps(List<Instance> ips, boolean ephemeral) {
        
        Set<Instance> toUpdateInstances = ephemeral ? ephemeralInstances : persistentInstances;
        
        HashMap<String, Instance> oldIpMap = new HashMap<>(toUpdateInstances.size());
        
        for (Instance ip : toUpdateInstances) {
            oldIpMap.put(ip.getDatumKey(), ip);
        }
        
        List<Instance> updatedIPs = updatedIps(ips, oldIpMap.values());
      ...省略代码,这里省略的代码为更新我们的ips
      toUpdateInstances = new HashSet<>(ips);
        if (ephemeral) {
            ephemeralInstances = toUpdateInstances;
        } else {
            persistentInstances = toUpdateInstances;
        }
}
```





  我们可以看到 Nacos 可以同时支持 AP 和 CP 两种模式, 我同一个集群里, 既可以有临时实例, 也可以有持久化实例, 而且持久化实例即使不在线, 也不会删除



  如果是临时实例，则 instance 不会在 Nacos 服务端持久化存储，需要通过上报心跳的方式进行包活，如果 instance 一段时间内没有上报心跳，则会被 Nacos 服务端摘除。在被摘除后如果又开始上报心跳，则会重新将这个实例注册。



  持久化实例则会持久化被 Nacos 服务端，此时即使注册实例的客户端进程不在，这个实例也不会从服务端删除，只会将健康状态设为不健康。



同一个服务下可以同时有临时实例和持久化实例，这意味着当这服务的所有实例进程不在时，会有部分实例从服务上摘除，剩下的实例则会保留在服务下。





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028355580IePcy.png)





# 2.Nacos 服务端介绍



## 2.1AP 模式下的 distro 协议



  distro 协议的工作流程如下：



- Nacos 启动时首先从其他远程节点同步全部数据。
- Nacos 每个节点是平等的都可以处理写入请求，同时把新数据同步到其他节点。
- 每个节点只负责部分数据，定时发送自己负责数据的校验值到其他节点来保持数据一致性。
- 当该节点接收到属于该节点负责的服务时，直接写入。
- 当该节点接收到不属于该节点负责的服务时，将在集群内部路由，转发给对应的节点，从而完成写入。
- 读取操作则不需要路由，因为集群中的各个节点会同步服务状态，每个节点都会有一份最新的服务数据。
- 而当节点发生宕机后，原本该节点负责的一部分服务的写入任务会转移到其他节点，从而保证 Nacos 集群整体的可用性。



  如下图, 每个 order 服务可以向任意一个 NacosServer 发起注册, Nacos 服务端集群每个节点都是平等的.



  例如我的 orderService 注册到 NacosServer2, 然后他先到会到 Server2 的 DistroFilter, 通过 distroHash(serviceName) % servers.size() 哈希当前的服务名取模计算出当前的 orderService 属于哪个节点, 假如计算出属于 NacosServer1, 则会路由到 NacosServer1 去注册, NacosServer1 注册完之后他会同步到 NacosServer2,3, 假如这个时候同步失败了, 会不断重试, 直到成功或者 NacosServer2,3 节点不存在了就结束.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028356118z2DFF.png)







  假设现在出现了分区, AP 模式的话不存在主节点的概念, 虽然 NacosAP 和 CP 可以共存, 如果是临时节点的情况下是不会触发 raft 协议同步数据只会用 distro 协议去作为集群的数据, 达到最终一致, 但是服务端同步数据只能 AP 或者 CP. 所以 AP 模式下不存在脑裂问题, 加速如下图, 出现了分区, 这个情况会损害可用性, 这个时候因为客户端可以从任意一个节点拉取数据并且缓存下来，客户端会表现为有时候服务存在有时候服务不存在, 等网络恢复了, 集群之间又开始同步数据达到最终一致.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102835678DvszZe.png)





## 2.2 Distro 协议中主要概念：



- Memer： 在 Nacos-Server 启动时，会在 cluster.conf 中配置整个集群列表，其作用是让每个节点都知道集群中的所有节点，该列表中的每一个节点都抽象成一个 Member
- MemberInfoReportTask： 在 Nacos-Server 启动成功后，会定时给除自己之外的其他 Member 进行通信，检测其他节点是否还存活。如果通信失败，会将该 Member 状态置为不健康的，如果后续和该节点重新通信成功，会将该节点的状态置为健康，该 Task 与 Responser 的计算密切相关
- Responser(对应下面 5.1 的 distroMapper 选择节点)： 对于每一个服务（比如：com.ly.OrderService）来说，在 Nacos-Server 集群中都会有一个专门的节点来负责。比如集群中有三个健康节点，这三个节点的 IP:Port 就是组成一个长度为 3 的 List<String>，对三个节点的 IP:Port 组成的 addressList 进行排序，这样在每一个节点中，addressList 的顺序都是一致的。这时 com.ly.OrderService 服务注册上来，会根据服务名计算对应的 hash 值，然后对集群的节点数取余获得下标，从 addressList 中获取对应的 IP:Port，这时这个 IP:Port 对应的节点就是该服务的 Responser，负责该服务的健康检查，数据同步，心跳维持，服务注册。如果客户端服务注册请求到了某个节点，但是本次注册的服务不是由该节点负责，会将该请求重定向到 responser 的节点去进行处理。注意： 这里的 addressList 是健康节点，一旦某个节点宕机或者网络发生故障，该节点会从 addressList 中移除，Service 对应的 Responser 会发生变化。
- HealthChecker： 对于每一个服务，都有一个 HealthCheck 去对其中的实例进行检测，检测的原则就是该实例最近上报心跳的时间与当前时间的时间差是否超过阈值，如果超过阈值，需要将该实例从服务中摘除。HealthCheck 在检测时，也会去进行 Responser 的检查，只有自己是当前服务的 Responser，才会去进行检测。
- DistroTask： 由于 Responser 规则的存在，对于某一个服务来说，只会有一个 node 来进行负责，那么其他的 node 是如何感知到非 responser 节点的服务数据的呢。DistroTask 就是做数据同步的，对当前自己持有的所有服务进行检测，只要有是自己 response 的，就把该服务的实例数据同步给其他 node。这里就有一个优化点，在同步数据时，并不是把该服务下所有的实例全部同步给其他节点，而是对该服务当前所有实例计算一个 checksum 值（减少传输的数据量，而且一般来说，实例变动是不频繁的）同步到其他节点。其他节点收到数据后，首先会检查同步过来的服务是否是由远端负责，如果是，比对自己节点中该服务的 checksum 值和远端的是否一致，如果不一致，请求远端节点获取最新的实例数据，再更新本地数据。
- LoadDataTask： 在节点刚启动时，会主动向其他节点拉取一次全量的数据，来让当前节点和整个集群中的数据快速保持一致。





## 2.2Nacos 注册中心是什么.



  还记得我们在 [SpringCloud Alibaba(一) Nacos 注册中心快速入门](https://blog.csdn.net/qq_24768941/article/details/122183355), 我们启动 nacos 的时候是执行了一个脚本.



  里面执行的命令启动了 target 下面的一个 jar





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102835729EfbaKp.png)





  然后看到 jar 里面的 MANIFEST.MF 文件, 看到启动类是 com.alibaba.nacos.Nacos





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102835769h3Wm5w.png)





  我们来到 com.alibaba.nacos.Nacos, 发现他本质上是一个 web 服务.

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028357973MGaEN.png)





##  2.3Nacos 服务端[源码](https://so.csdn.net/so/search?q=源码&spm=1001.2101.3001.7020)下载



-   idea 去 https://github.com/alibaba/nacos.git, 把源码拉取下来.

- git checkout tag or revision, 输入 1.4.1

- ![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_16,color_FFFFFF,t_70,g_se,x_16NC588H.png)

  ![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028358726jBQgZ.png)





# 2.4 本地启动 Nacos 服务端源码



###  2.4.1 编译 nacos-consistency



因为会提示有几个类缺失, 是因为这个包目录是由 protobuf 在编译时自动生成





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102835896AXzOCe.png)





 可以通过 mvn compile 来自动生成他们。如果使用的是 IDEA，也可以使用 IDEA 的 protobuf 插件





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102835940ISpZVH.png)







###  2.4.2 单机启动



  直接运行 console 模块里的 com.alibaba.nacos.Nacos.ja va 并且增加启动 vm 参数



-Dnacos.standalone=true





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102835974Gov0x9.png)





###  2.4.3 集群启动



  nacos 集群需要配置 mysql 存储，需要先创建一个数据，名字随便取，然后执行 distribution/conf 目录下的 nacos-mysql.sql 脚本，
然后修改 console\src\main\resources 目录下的 application.properties 文件里的 mysql 配置.



\### If use MySQL as datasource:
spring.datasource.platform=mysql
\### Count of DB:
db.num=1
\### Connect URL of DB:
db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoRecon
nect=true&useUnicode=true&useSSL=false&serverTimezone=UTC

db.user.0=root
db.password.0=roo



  运行 console 模块里的 com.alibaba.nacos.Nacos.java，需要增加启动 vm 参数端口号和实例运行路径 nacos.home(对应的目录需要自己提前创建好)，每台 server 的 nacos.home 目录里需要创建一个 conf 文件夹，里面放一个 cluster.conf 文件，文件里需要把所有集群机器 ip 和端口写入进去





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102836025ZV89G0.png)





  最后在 idea 的 service 里面进行配置





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_18,color_FFFFFF,t_70,g_se,x_1635e96I.png)

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_15,color_FFFFFF,t_70,g_se,x_161tpBrq.png)







![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102836152txSRYp.png)









# 3. 源码分析 



## 3.1 如何看源码



  我们第一次看源码, 最好的是看静态源码, 不要 debug, 而且第一次看只看主流程, 不要进去细枝末节去琢磨, 看到 if return 这些代码一般可以过掉, 实现类不知道是哪个可以根据上下文推测, 不行就打断点.



  源码分析前我在 Nacos github 看到的 https://github.com/alibaba/nacos/issues/7264, 我觉得这对我们看代码和平常的设计会有很大的启发, Nacos 源码里面大量用了异步和事件化机制, 去提高吞吐量, 并且解耦.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028362568KJbnB.png)







## 3.2Nacos 服务列表结构



  我们在 [SpringCloud Alibaba(三) Nacos 客户端源码浅析 (AP 架构)_Dr. 劳的博客 - CSDN 博客](https://blog.csdn.net/qq_24768941/article/details/122334104), 看到了客户端调用了 Nacos 服务端几个接口, 官网 Nacos[Open API 指南](https://nacos.io/zh-cn/docs/open-api.html)



- POST /v1/ns/instance 注册实例
- PUT /v1/ns/instance/beat 心跳保活
- GET /v1/ns/instance/list 获取服务列表 获取我们的 ServiceMap 里面的数据



这里我们提前说一下, 服务端保存的服务列表是一个 Map 结构 ServiceMap





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102836293RSmeuT.png)





- Map<namespace, Map<group::serviceName, Service>>, 这样做是为了环境隔离
- Service 里面有一个 Map<String, Cluster> clusterMap
- Cluster 里面有两个集合 Set<Instance> persistentInstances, 持久化集合, 临时实例集合 Set<Instance>ephemeralInstances.
- Instance 里面存储的是这个节点的属性, ip,port,lastHeatbeat, 等等属性





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-2022041210283644594Nhc9.png)







## 3.3 POST /v1/ns/instance 注册实例 (AP 模式, 临时实例)



  我们找到他是一个 register 方法, 调用了 SeriveManager#registerInstance, 并且客户端注册过来的 serviceName 会带有 group 的拼接





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102836658fvvMVZ.png)







###  3.3.1 ServiceManager#registerInstance



  首先创建了一个空的 Service 存储到 ServiceMap 里面, 然后调用 addInstance





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102836752Ztvclm.png)





###  3.3.2 SeriveManager#addInstance



  这里的 buildInstanceListKey 他会根据是否临时实例, 拼接一个新的 key, 客户端默认是 ap 模式注册 +



- 临时实例: com.alibaba.nacos.naming.iplist.ephemeral.public##DEFAULT_GROUP@@mall-user
- 持久化实例 com.alibaba.nacos.naming.iplist.public##DEFAULT_GROUP@@mall-user





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102836822Izkf7K.png)





###  3.3.3 DistroConsistencyServiceImpl#put



  因为我们讲的 AP [架构](https://so.csdn.net/so/search?q=架构&spm=1001.2101.3001.7020)所以肯定进的 Distro, 如果不知道可以打断点. 这个方法做了两件事



- onput 方法, 注册实例
- sync 方法, 将注册完的实例同步到其他 Nacos 集群的节点





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837018jaDinD.png)







![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837087KTJSNA.png)





  而 put 方法他只是封装了一个 datum 放进 dataStore,dataStore 是 Nacos 用来缓存客户端节点数据, 并且往一个 map 里面 put 了一下, 然后 addTask, 他只做了一件事, 把这个 key 放进一个 BlockingQueue, 方法结束, 然后直接返回给客户端注册成功, 很明显他是异步执行的. 我们应该看 task 的 run 方法做了什么.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837145bOcKbs.png)







![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837241OLYLWu.png)





###  3.3.4 DistroConsistencyServiceImpl#run



  死循环从阻塞队列获任务并且执行 DistroConsistencyServiceImplhandler, 注意他是单线程的, 就算并发他也是单线程去执行完一个任务再到下一个.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837287wsoJEP.png)





###  3.3.5 DistroConsistencyServiceImpl#handler



  这里发现会根据 action 调用 onChange 或者 onDelete, 我们猜测, 有注册, 肯定就有删除注册实例的方法.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837371Aq2N6S.png)





###   3.3.6 Service#updateIps



  上面的 onChange 会调用到 Service#onChange, 这里有一个关键的方法 updateIps





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837466nwHVoj.png)





 Service#updateIps 重点方法在于 clusterMap.get(entry.getKey()).updateIPs(entryIPs, ephemeral), 去更新我们的 ServiceMap, 并且使用 udp 推送给其他服务, 告诉我们的客户端节点, 我更新了服务列表.



```
//ServiceChangeEvent会触发这个方法
 @Override
    public void onApplicationEvent(ServiceChangeEvent event) {
        Service service = event.getService();
        String serviceName = service.getName();
        String namespaceId = service.getNamespaceId();
        
        Future future = GlobalExecutor.scheduleUdpSender(() -> {
            try {
                Loggers.PUSH.info(serviceName + " is changed, add it to push queue.");
                //获取需要推送的client
                ConcurrentMap<String, PushClient> clients = clientMap
                        .get(UtilsAndCommons.assembleFullServiceName(namespaceId, serviceName));
                
                Map<String, Object> cache = new HashMap<>(16);
                long lastRefTime = System.nanoTime();
                for (PushClient client : clients.values()) {
                    if (client.zombie()) {
                        Loggers.PUSH.debug("client is zombie: " + client.toString());
                        clients.remove(client.toString());
                        Loggers.PUSH.debug("client is zombie: " + client.toString());
                        continue;
                    }
                    
                    Receiver.AckEntry ackEntry;
                    Loggers.PUSH.debug("push serviceName: {} to client: {}", serviceName, client.toString());
                    String key = getPushCacheKey(serviceName, client.getIp(), client.getAgent());
                    //省略代码
                    //for循环推送给对应的服务
                    udpPush(ackEntry);
                }
            } catch (Exception e) {
                Loggers.PUSH.error("[NACOS-PUSH] failed to push serviceName: {} to client, error: {}", serviceName, e);
                
            } finally {
                futureMap.remove(UtilsAndCommons.assembleFullServiceName(namespaceId, serviceName));
            }
        }, 1000, TimeUnit.MILLISECONDS);
        
        futureMap.put(UtilsAndCommons.assembleFullServiceName(namespaceId, serviceName), future);
        
    }
```



###  3.3.7 Cluster#updateIps  CopyOnWrite 机制



  上面的 Service#UpdateIps 会调用到 Cluster#updateIps, 这个方法其实最终就是更新我们 ServiceMap(服务列表) 里面的 Set<Instance>ephemeralInstances.



  如果我们直接修改 ephemeralInstances, 他里面的 Instance 对象状态会不断的变化, 这里用了写时复制的思想, 先写到一个临时变量, 修改为最终版的时候, 最后再替换我们原来的变量. 但是我们同一个实例很多客户端同一时间注册的时候, 不就乱套了吗? 复制了多个临时变量一直在覆盖, 客户端集群同一时间多个机器如果这个时候获取了这个 ephemeralInstances, 会里面的 Instance 不就变来变去.



- 首先我们上面说了阻塞队列获取任务是单线程的所以同一个服务多个实例注册过来不会同时修改我们的 ephemeralInstances, 就算不同的服务注册进来, 也只会单线程的修改.
- 那么我们修改 ephemeralInstances 的时候直接加一把锁不就好了吗, 修改的时候客户端就不能获取, 这样我们服务端获取的时候都是一致的. 那这不是就降低了我们并发. 但是这里的代码避开了加锁这个问题. 他在修改的时候, 先将老的 ephemeralInstances 数据赋值给我们的一个 oldMap, 这个 ips 里面有我们所有的最新的 Instance, 然后和我们的 oldMap 去做比较, 如果是新增的节点就添加进 oldMap, 如果是删除节点就删除 oldMap 的节点, 如果是修改节点状态, 就 new 一个新 Instance 的对象替换我们的 oldMap 里面的对象. 直到修改完成最后返回修改完后的 ips, 然后直接替换 ephemeralInstances. 这样我们就可以不用加锁, 然后修改的时候也不会影响客户端, 导致我一个 order client 的集群, 同一时间去拉取我们的服务列表 ServiceMap, 然后服务列表的状态不一致的问题.



```
/**
     * Create a beat for instance.
     *
     * @param request http request
     * @return detail information of instance
     * @throws Exception any error during handle
     */
    @CanDistro
    @PutMapping("/beat")
    @Secured(parser = NamingResourceParser.class, action = ActionTypes.WRITE)
    public ObjectNode beat(HttpServletRequest request) throws Exception {
        //前面省略代码
        //根据上传的心跳去找对应的实例,找不到就new一个,重新走注册逻辑,可以看上面的注册逻辑
        if (instance == null) {
            if (clientBeat == null) {
                result.put(CommonParams.CODE, NamingResponseCode.RESOURCE_NOT_FOUND);
                return result;
             }
            instance = new Instance();
            instance.setPort(clientBeat.getPort());
            //前面省略代码
            serviceManager.registerInstance(namespaceId, serviceName, instance);
        }
        //前面省略代码
        Service service = serviceManager.getService(namespaceId, serviceName);
        //处理心跳
        service.processClientBeat(clientBeat);
        result.put(CommonParams.CODE, NamingResponseCode.OK);
        if (instance.containsMetadata(PreservedMetadataKeys.HEART_BEAT_INTERVAL)) {
            result.put(SwitchEntry.CLIENT_BEAT_INTERVAL, instance.getInstanceHeartBeatInterval());
        }
        result.put(SwitchEntry.LIGHT_BEAT_ENABLED, switchDomain.isLightBeatEnabled());
        return result;
    }
```





### 3.3.8PushService#udpPush



  更新完节点列表会触发一个 ServiceChangeEvent 事件, Spring 会执行对应的 onApplicationEvent 方法, 最后会触发 PushService#udpPush, 这个方法就是异步 upd 通知 Nacos 客户端, 推送失败会重复一定次数, 我新注册了一个 client. 这样客户端就不需要等到 15 秒定时拉取, 也能知道服务列表有改变.



  Nacos 的这种推送模式 比起 ZK 的 TCP 方式来说 节约了资源, 增强了效率 Client 收到 Server 的消息之后 会给 Server 一个 ACK, 如果一定时间内 Server 没有接收到 ACK, 会进行重发 当超过重发之后, 不再重发 尽管 UDP 会丢包, 但是仍然有定时任务的轮训兜底 不怕丢了数据 





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028375032YoPko.png)

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028375510Rpt9U.png)





```
@Override
    public void run() {
        try {
            //这里放后面讲
            if (!getDistroMapper().responsible(service.getName())) {
                return;
            }
            
            if (!getSwitchDomain().isHealthCheckEnabled()) {
                return;
            }
            //获取当前Nacos服务上的所有节点
            List<Instance> instances = service.allIPs(true);
            //检测心跳
            // first set health status of instances:
            for (Instance instance : instances) {
                if (System.currentTimeMillis() - instance.getLastBeat() > instance.getInstanceHeartBeatTimeOut()) {
                    if (!instance.isMarked()) {
                        if (instance.isHealthy()) {
                            //如果一段时间没心跳,设置健康状态为false
                            instance.setHealthy(false);
                            Loggers.EVT_LOG
                                    .info("{POS} {IP-DISABLED} valid: {}:{}@{}@{}, region: {}, msg: client timeout after {}, last beat: {}",
                                            instance.getIp(), instance.getPort(), instance.getClusterName(),
                                            service.getName(), UtilsAndCommons.LOCALHOST_SITE,
                                            instance.getInstanceHeartBeatTimeOut(), instance.getLastBeat());
                            getPushService().serviceChanged(service);
                            ApplicationUtils.publishEvent(new InstanceHeartbeatTimeoutEvent(this, instance));
                        }
                    }
                }
            }
            
            if (!getGlobalConfig().isExpireInstance()) {
                return;
            }
            
            // then remove obsolete instances:
            for (Instance instance : instances) {
                
                if (instance.isMarked()) {
                    continue;
                }
                //如果30秒没有心跳,则剔除节点
                if (System.currentTimeMillis() - instance.getLastBeat() > instance.getIpDeleteTimeout()) {
                    // delete instance
                    Loggers.SRV_LOG.info("[AUTO-DELETE-IP] service: {}, ip: {}", service.getName(),
                            JacksonUtils.toJson(instance));
                    //发送删除节点请求
                    deleteIp(instance);
                }
            }
            
        } catch (Exception e) {
            Loggers.SRV_LOG.warn("Exception while processing client beat time out.", e);
        }
        
    }
```





## 3.4 distroProtocol#sync---Nacos 集群同步新增的节点



  上面我们看到的实际逻辑就是直接往队列里面放一个任务, 然后异步修改了 ServiceMap, 那么修改完, 我 Nacos 是一个集群啊, 我其他 Nacos 节点怎么同步你当前节点的数据呢? 所以我们看回之前的 put 方法, 他的 distroProtocol#sync 方法就是为了集群同步新增节点数据.



- sync 方法首先去 MemberManager 去找到所有的 Nacos 服务端节点, 然后排除掉自己, 因为自己已经修改了 ServiceMap
- 新增同步节点延时任务
- 由于这里连续调用了两个线程池执行异步任务最终会调用到 DistroSyncChangeTask, 中间找的地方特别难找, 所以直接看到 DistroSyncChangeTask.
- DistroSyncChangeTask 的 run 方法是发送 http 请求去其他节点最终调用到 NamingProxy#sync(这个类我们客户端也看到过, 专门用于 http 请求同步数据), 最终发送 PUT 请求到 /v1/ns/distro/datum, 如果发送了就把当前任务丢进一个延时队列, 无限循环执行, 直到成功, 或者需要发送的 Nacos 服务端不在线. 这样就可以保证我们集群的最终一致. 并且我们可以发现, Nacos 服务端集群之间能彼此感知对应的存在, 所以他们之间肯定还会有个心跳任务.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837087KTJSNA.png)

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837608EIHG2S.png)





  最终会调用到 DistroSyncChangeTask#run

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837678qhNK0X.png)

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837776D5HvvX.png)







###  3.4.1 PUT /v1/ns/distro/datum 集群间同步数据



  上面说到 Nacos 客户端请求去服务端之后, 服务端会调用 PUT /v1/ns/distro/datum, 这个接口去同步新增的节点数据.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837864yxDDSK.png)





   上面的 onReceive 会调用 DistroConsistencyServiceImpl#processData, 然后 DistroConsistencyServiceImpl#onPut 方法和上面新增节点方法一模一样, 这里就不说了, 就是最终修改 ServiceMap, 只不过不需要在同步去其他 Nacos 服务端的节点.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837900gCKW3G.png)







![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102837946ovcrSa.png)







## 3.5 GET /v1/ns/instance/list



  上面说了服务注册就是在修改 ServiceMap, 那获取服务列表肯定是获取 ServiceMap.



### 3.5.1GET /v1/ns/instance/list



  直接找到这个接口





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838049jtBi1t.png)





###   InstanceController#doSrvIpxt



  我们可以看到这个方法其实就是去 ServiceMap 获取对应的信息, 然后返回, 后面的逻辑有兴趣可以自己看一下.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028381720bP0z1.png)





中间省略一大串代码

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838232iHOSfy.png)





  getService, 去 serviceMap, 获取信息

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838270RIzigI.png)





##  3.6 PUT /v1/ns/instance/beat 心跳保活



  我们知道 Nacos 客户端会有个定时任务, 定时请求服务端的这个接口 PUT /v1/ns/instance/beat, 去告诉服务端我还活着.





### 3.6.1 PUT /v1/ns/instance/beat



- 根据心跳去当前的 ServiceMap 找这个 Instance, 如果找不到就重新走上面的注册逻辑, 不懂可以看一下上面的 serviceManager#registerInstance, 这里可以很好的解释了, 我为什么注册接口是异步的, 如果失败了呢?



​      \1. 注册接口逻辑太长, 所以异步



​      \2. 即使注册失败了, 心跳上来也会重新注册. 达到最终一致



- 调用 service#processClientBeat(clientBeat); 处理心跳



```
/**
 * Distro filter.
 *
 * @author nacos
 */
public class DistroFilter implements Filter {
 
    
    @Override
    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain)
            throws IOException, ServletException {
            // proxy request to other server if necessary:
            if (method.isAnnotationPresent(CanDistro.class) && !distroMapper.responsible(groupedServiceName)) {
                //省略代码
                //根据ServiceName,hash找到我这个客户端应该被集群中哪个Nacos服务端去处理
                final String targetServer = distroMapper.mapSrv(groupedServiceName);
                
                List<String> headerList = new ArrayList<>(16);
                Enumeration<String> headers = req.getHeaderNames();
                while (headers.hasMoreElements()) {
                    String headerName = headers.nextElement();
                    headerList.add(headerName);
                    headerList.add(req.getHeader(headerName));
                }
                //计算出targetServer,直接请求去targetServer
                RestResult<String> result = HttpClient
                        .request("http://" + targetServer + req.getRequestURI(), headerList, paramsValue, body,
                                PROXY_CONNECT_TIMEOUT, PROXY_READ_TIMEOUT, Charsets.UTF_8.name(), req.getMethod());
                String data = result.ok() ? result.getData() : result.getMessage();
}
        
    }
```



###  3.6.2 ClientBeatProcessor#run



  上面的处理心跳 service#processClientBeat,new 了一个 ClientBeatProcessor, 然后丢进线程池, 立即执行, 所以我们看一下 ClientBeatProcessor#run, 他做了以下事情



- 设置最后一次心跳时间, Nacos 服务端肯定还有一个定时任务去扫描这个时间, 去服务端的状态修改.
- 当实例状态由不健康变为健康, 设置 healthy 属性, 并且发送 udp 请求, 告诉其他 client 节点.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838311Ocu3zn.png)







![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838399skNHnu.png)





# 4.Nacos 心跳保活客户端任务



  上面主要介绍了服务端几个主要暴露的接口, 我们还遗漏了一个点, 客户端心跳上来, 我们服务端还需要一个定时任务, 去看一下哪一些服务是有心跳的, 哪一些没心跳需要下线.



## 4.1 ClientBeatCheckTask



   上面我们注册的时候调用了 ServiceManager#registerInstance 首先创建了一个空的 Service 存储到 ServiceMap 里面, 然后调用 addInstance.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102836752Ztvclm.png)





### 4.1.1 Service#init



  上面的 ServiceManager#createEmptyService 会绑定 listen 并且最后会调用到 Service#init, 初始化一个定时任务 ClientBeatCheckTask, 所以我们得看他的 run 方法





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838447x5M7eQ.png)







![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838499FQMIL4.png)







### 4.1.2 ClientBeatCheckTask#run



  这里就是心跳任务, 去检查当前节点客户端的心跳.



- 如果已经一段时间没心跳, 则设置健康状态为 false, 并且推送 udp 给我们的客户端其他节点
-  如果已经 30 秒没有收到客户端的心跳, 则会调用 deleteIp(instance); 调用这个方法 http 请求, 去其他 Nacos 服务端, 修改该节点的信息, 调用的 DELEET /v1/ns/instance, 这个接口实际上就是调用了新增的 put 方法, 一模一样, 就是修改 ServiceMap, 去剔除节点, 并且广播给其他 Nacos 服务节点, 这里不再展开说.



```
@Override
    public void run() {
        try {
            //这里放后面讲
            if (!getDistroMapper().responsible(service.getName())) {
                return;
            }
            
            if (!getSwitchDomain().isHealthCheckEnabled()) {
                return;
            }
            //获取当前Nacos服务上的所有节点
            List<Instance> instances = service.allIPs(true);
            //检测心跳
            // first set health status of instances:
            for (Instance instance : instances) {
                if (System.currentTimeMillis() - instance.getLastBeat() > instance.getInstanceHeartBeatTimeOut()) {
                    if (!instance.isMarked()) {
                        if (instance.isHealthy()) {
                            //如果一段时间没心跳,设置健康状态为false
                            instance.setHealthy(false);
                            Loggers.EVT_LOG
                                    .info("{POS} {IP-DISABLED} valid: {}:{}@{}@{}, region: {}, msg: client timeout after {}, last beat: {}",
                                            instance.getIp(), instance.getPort(), instance.getClusterName(),
                                            service.getName(), UtilsAndCommons.LOCALHOST_SITE,
                                            instance.getInstanceHeartBeatTimeOut(), instance.getLastBeat());
                            getPushService().serviceChanged(service);
                            ApplicationUtils.publishEvent(new InstanceHeartbeatTimeoutEvent(this, instance));
                        }
                    }
                }
            }
            
            if (!getGlobalConfig().isExpireInstance()) {
                return;
            }
            
            // then remove obsolete instances:
            for (Instance instance : instances) {
                
                if (instance.isMarked()) {
                    continue;
                }
                //如果30秒没有心跳,则剔除节点
                if (System.currentTimeMillis() - instance.getLastBeat() > instance.getIpDeleteTimeout()) {
                    // delete instance
                    Loggers.SRV_LOG.info("[AUTO-DELETE-IP] service: {}, ip: {}", service.getName(),
                            JacksonUtils.toJson(instance));
                    //发送删除节点请求
                    deleteIp(instance);
                }
            }
            
        } catch (Exception e) {
            Loggers.SRV_LOG.warn("Exception while processing client beat time out.", e);
        }
        
    }
```



  DELETE /v1/ns/instance, 剔除节点

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838578tRScKQ.png)





### 4.1.3 ServiceManager#init 同步客户端服务状态



  上面心跳对比更改了 healthy 属性, 然后 30 秒没收到心跳剔除节点, 那么如果服务下线, 没到 30 秒, 我们在哪里同步给其他 Nacos 服务端呢?



  我们看到 ServiceManager 有一个 init 方法, 并且有 @PostConstruct, 证明 Spring 启动的时候会调用这个方法, 并且 5 秒调用一次. 首先判断数据在哪个节点处理 (distroMapper.responsible(groupedServiceName) 可以看第 5 点), 如果是当前节点处理, 这个方法会将客户端服务信息包括健康状态拼接成字符串, 同步给其他 Nacos 服务端, 这样我们就能感应到客户端节点状态的改变.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838643LSbn6A.png)





  拼接需要发送的数据, 包括客户端服务信息和服务的当前状态

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838710z7GRr1.png)







### 4.1.4 ServiceStatusSynchronizer#send



  上面的方法会拼接最终会调用到 ServiceStatusSynchronizer#send, 发送 http 请求, POST



/v1/ns/service/status, 但是失败不重试.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-2022041210283877324fqqn.png)







### 4.1.5 POST /v1/ns/service/status 同步客户端服务状态 





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102838880tAdvcm.png)





### 4.1.6 ServiceManager#updatedHealthStatus 



  上面的接口最后会异步调用到 ServiceManager#updatedHealthStatus, 里面会获取发送过来的节点数据, 更新 healthy 状态等.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028389552lVkwN.png)

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102839108ZASROY.png)









# 5. DistroFilter 和 @CanDistro Nacos 服务端路由



  之前我们提到 AP 模式下, distro 协议对于每一台机器都是平等的, 会请求到任意一台机器, 如果我的 Order 服务, 第一次心跳被 Nacos1 处理, 第二次心跳被 Nacos2 处理, 但是我们看了上面心跳接口的逻辑是没有同步给其他 Nacos 服务端节点的, 那么整个集群的心跳是不是乱的, 这样我的心跳保活客户端任务怎么判断?



##  5.1DistroFilter



  实际上通过我们的观察所有对于 ServiceMap 的修改的接口上面都带了一个 @CanDistro 的注解. 然后我们全局搜这个注解发现他在 DistroFilter 里面使用到了, Filter 会在我们请求之前拦截.



DistroFilter#mapSrv



  根据服务名称 Hash, 再对我们服务器数量求模, 计算出我当前的服务应该属于哪一台 Nacos 去处理. 这个 Servers.size 是动态的, Nacos 服务端集群之间会有心跳去保活. 如果不属于我这台 Nacos 服务去处理, 则路由到其他服务, 否则由本服务处理. 这里看出为了减轻单节点压力, 对于 Nacos 客户端请求都是分散在集群的节点中.



distroMapper#responsible(groupedServiceName)



  这个方法是判断当前实例是否当前节点处理. 但是这样做有一个缺点就是:Nacos 如果扩容，或者某个节点宕机，所有的 service 都可能会迁移到别的机器, 会很耗费资源. 据说 2.0 使用 grpc 长连接解决了这个问题.



```
/**
 * Distro filter.
 *
 * @author nacos
 */
public class DistroFilter implements Filter {
 
    
    @Override
    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain)
            throws IOException, ServletException {
            // proxy request to other server if necessary:
            if (method.isAnnotationPresent(CanDistro.class) && !distroMapper.responsible(groupedServiceName)) {
                //省略代码
                //根据ServiceName,hash找到我这个客户端应该被集群中哪个Nacos服务端去处理
                final String targetServer = distroMapper.mapSrv(groupedServiceName);
                
                List<String> headerList = new ArrayList<>(16);
                Enumeration<String> headers = req.getHeaderNames();
                while (headers.hasMoreElements()) {
                    String headerName = headers.nextElement();
                    headerList.add(headerName);
                    headerList.add(req.getHeader(headerName));
                }
                //计算出targetServer,直接请求去targetServer
                RestResult<String> result = HttpClient
                        .request("http://" + targetServer + req.getRequestURI(), headerList, paramsValue, body,
                                PROXY_CONNECT_TIMEOUT, PROXY_READ_TIMEOUT, Charsets.UTF_8.name(), req.getMethod());
                String data = result.ok() ? result.getData() : result.getMessage();
}
        
    }
```



###  DistroFilter#mapSrv



  根据服务名称 Hash, 再对我们服务器数量求模, 计算出我当前的服务应该属于哪一台 Nacos 去处理. 这个 Servers.size 是动态的, Nacos 服务端集群之间会有心跳去保活.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102839217XEX2Xe.png)





##  5.2 ClientBeatCheckTask#run



  我们再看回上面的对客户端的心跳保活任务, 这个 DistroMapper#responsible, 会计算服务名出当前定时任务需不需要执行, 假如 order 服务是我这台 nacos 服务执行的, 就执行心跳保活任务, 并且如果状态改变, 同步给其他服务, 如果不是我这台服务执行, 则 return;





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102839553LFGlIa.png)





DistroMapper#responsible





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102839636VPKMRB.png)







# 6. Nacos 服务端之间的心跳任务



  . 上面我们说到如果是更改 serviceMap 的请求, 会根据服务名称路由到对应的服务节点去处理. 他根据服务名称 hash 再模服务器数量, 既然这里有服务器数量这个变量, 我们 Nacos 服务直接必定还有一个心跳任务, 不然服务与服务直接怎么能知道相互之间的状态, 万一有一台 Nacos 服务挂了, 这个服务器数量变量也需要变.



## 6.1serverListManager#init



  Spring 启动的时候会执行 @PostConstruct 的方法, 所以会执行 serverListManager#init, 里面往线程池放了一个任务 ServerStatusReporter,2 秒执行一次, 去同步 Nacos 服务端之间的状态





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102839919azqryB.png)





### 6.1.1 ServerStatusSynchronizer#send



  上面的定时任务最终会调用到 ServerStatusSynchronizer#send, 发送 http 请求



GET /v1/ns/operator/server/status





### 6.1.2 GET /v1/ns/operator/server/status



  这个接口以后将会被移除, 这个接口主要用于更新 Nacos 服务的最新一次心跳, 并且更新服务信息





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102840050nExvSl.png)







### 6.1.3 ServerMemberManager#MemberInfoReportTask, 服务端心跳保活任务



  单单有心跳了还不行, 我们还需要一个定时任务去维护这个心跳, 把一段时间没有心跳的节点剔除掉, 我们会发现 ServerMemberManager 初始化的时候初始化了一个 MemberInfoReportTask, 并且在 ServerMemberManager#onApplicationEvent, 触发了 tomcat 启动完成的一个方法, 里面判断, 如果不是以单节点启动, 就定时执行 ServerMemberManager#MemberInfoReportTask





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102840239TcAfBl.png)







![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102840370RAaJqQ.png)





###  6.1.4 MemberInfoReportTask



  MemberInfoReportTask 继承了 task, 所以每一次执行完都会执行 after 方法, MemberInfoReportTask 重写了 after 方法, 每 2 秒执行一次, 去请求除了自己的节点的所有 Nacos 服务端节点, 如果成功则比对 Nacos 服务端其他节点有没有更变状态, 失败则重试, 一定次数之后剔除对应的 Nacos 服务端节点, 并且修改触发 MembersChangeEvent 事件, 然后触发 DistroMapper#onEvent 事件, DistroMapper 就是用于计算服务节点的





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102840462HmvQWU.png)

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102840607vbajtH.png)





DistroMapper#onEvent





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102840724pd7Bs5.png)







# 7.Nacos 服务端启动拉取数据与扩容



  当我有一台 Nacos 服务端挂了, 然后再启动, 或者有一台新的节点加入我的集群, 新加入节点需要手动修改我们的 cluster.conf 文件, 一旦修改了, Nacos 服务会监控到这个文件的修改我们的 member; 我新节点肯定需要去拉取数据, 这个时候如果某一些客户端节点在注册, 我可能拉取的是老数据, 但是没关系, 客户端注册完之后还是需要对每一个 member 进行广播, 并且还有一个 DistroVerifyExecuteTask, 这个任务会不断往其他 Nacos 服务按节点发送我当前负责的节点, 其他节点收到后, 会进行校验和同步修改, 达到最终一致;



  例如我 Nacos 1 只负责处理 order 服务, Nacos2 负责处理 user 服务, Nacos3 负责 product 和 store 服务的处理, 各自节点上面的数据肯定是完整的, 就算数据丢失或者不完整, 客户端心跳上来也会补齐, 即使 Nacos 现在进行扩容或者索容了, 期间产生数据的不一致, 后面客户端心跳上来也会补齐对应节点的数据. 现在多了 Nacos4, 可能就由 Nacos4 处理 store 服务了, 这样所有 store 服务的注册心跳都转移到了 Nacos4, 现在 Nacos 1 负责处理 order 服务, Nacos2 负责处理 user 服务, Nacos3 负责 product,Nacos4 负责 store 服务, 每一个节点他们负责的客户端服务的数据肯定是最新的.



  所以每个 Nacos 节点, 会把自己处理的数据同步到别的节点, 别的节点就可以根据同步的数据进行校 验, 然后修改和同步. DistroVerifyExecuteTask 可以参考 [Nacos 2.0 源码分析 - Distro 协议详解 - 不会发芽的种子 - 博客园](https://www.cnblogs.com/lukama/p/14918667.html#_caption_h3_1)



对应上面标题 2 的每个节点只负责部分数据，定时发送自己负责数据的校验值到其他节点来保持数据一致性。





## 7.1 DistroProtocol#startLoadTask 服务启动拉取节点任务



  当我们服务启动的时候我们会注入 DistroProtocol 这个 bean, 他的构建方法会调用 DistroProtocol#startLoadTask.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102840868m88Lmq.png)





###  7.1.1 DistroLoadDataTask#run



  上面那个方法最终会调用 DistroLoadDataTask#run, 然后调用 DistroLoadDataTask#loadAllDataSnapshotFromRemote, 最终会调用 NamingProxy#getAllData,http 请求去其中一个 Nacos 服务节点拉取客户端的服务列表,



GET v1/ns/distro/datums 然后保存到本地 return.





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102841022sRLQVK.png)





DistroLoadDataTask#loadAllDataSnapshotFromRemote

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-202204121028411514JEdbN.png)







![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102841211IJfkUb.png)

  NamingProxy#getAllData,http 请求去其中一个 Nacos 服务节点拉取客户端的服务列表调用 GET /v1/ns/distro/datums

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102841316YJpbkI.png)







##  7.2 GET /v1/ns/distro/datums



   拉取本地的内存中的服务列表节点返回

![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102841414Q2m7Wz.png)







# 8. 附上整体源码的阅读流程图 





![img](http://cdn.jayh.club/uPic/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBARHIu5Yqz,size_20,color_FFFFFF,t_70,g_se,x_16-20220412102841955h8k8z9.png)







#  9. 自我总结



- 里面用了很多异步, 模块化, 队列, 使代码解耦和提高吞吐量, 这是我们平常写业务代码所涉及不了的知识盲点
- DistroMapper 很巧妙的解决了没有 Leader 下, 每个节点都是平等的, 但是不同的客户端根据路由到不同的 Nacos 服务端去处理, 减轻了单点的压力
- 但是 Nacos 服务端扩容，或者某个节点宕机，所有的 service 都可能会迁移到别的机器, 这个时候可能会有一定的资源损耗.
- Distro 通过不断的重试和定时任务, 使整个集群达到最终一致
- 熟悉了一个注册中心他的整体流程, 他所需要的功能
- 服务端更新 serviceMap 的时候写时复制真是一种巧妙的设计
- 自己读过一遍的源码, 再去细细品味, 而且读提高了自己的源码能力, 能从很多切入点去知道源码他做了什么事情.
- 源码阅读真是难度挺大, 希望自己可以坚持, 这篇博文应该也没啥人能看到最后, 当做自己第一次深入阅读开源中间件源码的一个开始.





参考:[Nacos AP 模型原理。_无能力者只知抱怨 - CSDN 博客_ap 模型](https://blog.csdn.net/Horizon_Zy/article/details/115655436?spm=1001.2014.3001.5501)



​    [nacos 高可用（图解 + 秒懂 + 史上最全）_架构师尼恩 - CSDN 博客_nacos 高可用模式](https://blog.csdn.net/crazymakercircle/article/details/120702536)



[    Nacos 2.0 源码分析 - Distro 协议详解 - 不会发芽的种子 - 博客园](https://www.cnblogs.com/lukama/p/14918667.html)

